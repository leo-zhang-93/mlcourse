#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin /Users/drosen/Dropbox/repos/mlcourse/Lectures/
\textclass beamer
\begin_preamble
\usetheme{CambridgeUS} 
\beamertemplatenavigationsymbolsempty


% Set Color ==============================
\definecolor{NYUPurple}{RGB}{87,6,140}
\definecolor{LightPurple}{RGB}{165,11,255}


\setbeamercolor{title}{fg=NYUPurple}
%\setbeamercolor{frametitle}{fg=NYUPurple}
\setbeamercolor{frametitle}{fg=NYUPurple}

\setbeamercolor{background canvas}{fg=NYUPurple, bg=white}
\setbeamercolor{background}{fg=black, bg=NYUPurple}

\setbeamercolor{palette primary}{fg=black, bg=gray!30!white}
\setbeamercolor{palette secondary}{fg=black, bg=gray!20!white}
\setbeamercolor{palette tertiary}{fg=gray!20!white, bg=NYUPurple}

\setbeamertemplate{headline}{}

\setbeamercolor{parttitle}{fg=NYUPurple}
\setbeamercolor{sectiontitle}{fg=NYUPurple}
\setbeamercolor{sectionname}{fg=NYUPurple}
\setbeamercolor{section page}{fg=NYUPurple}

\AtBeginSection[]{
  \begin{frame}
  \vfill
  \centering
\setbeamercolor{section title}{fg=NYUPurple}
 \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}\usebeamercolor[fg]{title}\insertsectionhead\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
}
\end_preamble
\options aspectratio=169,handout
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "eulervm" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\pdf_quoted_options "allcolors=NYUPurple,urlcolor=LightPurple"
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 0
\use_package esint 0
\use_package mathdots 0
\use_package mathtools 0
\use_package mhchem 0
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\boxbgcolor #ff31d8
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\reals}{\mathbf{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\integers}{\mathbf{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\naturals}{\mathbf{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rationals}{\mathbf{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ca}{\mathcal{A}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cb}{\mathcal{B}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cc}{\mathcal{C}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cd}{\mathcal{D}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ce}{\mathcal{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cf}{\mathcal{F}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cg}{\mathcal{G}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ch}{\mathcal{H}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ci}{\mathcal{I}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cj}{\mathcal{J}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ck}{\mathcal{K}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cl}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cm}{\mathcal{M}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cn}{\mathcal{N}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\co}{\mathcal{O}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cp}{\mathcal{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cq}{\mathcal{Q}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calr}{\mathcal{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cs}{\mathcal{S}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ct}{\mathcal{T}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cu}{\mathcal{U}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cv}{\mathcal{V}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cw}{\mathcal{W}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cx}{\mathcal{X}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cy}{\mathcal{Y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cz}{\mathcal{Z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ind}[1]{1(#1)}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
pr}{P}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\pr}{\mathbb{P}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\predsp}{\cy}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%{
\backslash
hat{
\backslash
cy}}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\outsp}{\cy}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\prxy}{P_{\cx\times\cy}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prx}{P_{\cx}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\prygivenx}{P_{\cy\mid\cx}}
\end_inset

 
\begin_inset ERT
status collapsed

\begin_layout Plain Layout

%
\backslash
newcommand{
\backslash
ex}{E}
\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset FormulaMacro
\newcommand{\ex}{\mathbb{E}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\var}{\textrm{Var}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\cov}{\textrm{Cov}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sgn}{\textrm{sgn}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\sign}{\textrm{sign}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\kl}{\textrm{KL}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\law}{\mathcal{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eps}{\varepsilon}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\as}{\textrm{ a.s.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\io}{\textrm{ i.o.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ev}{\textrm{ ev.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\convd}{\stackrel{d}{\to}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\eqd}{\stackrel{d}{=}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\del}{\nabla}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\loss}{\ell}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\risk}{R}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emprisk}{\hat{R}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\lossfnl}{L}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\emplossfnl}{\hat{L}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\empminimizer}[1]{\hat{#1}^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\minimizer}[1]{#1^{*}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\optimizer}[1]{#1^{*}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\etal}{\textrm{et. al.}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\tr}{\operatorname{tr}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\trace}{\operatorname{trace}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\diag}{\text{diag}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rank}{\text{rank}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\linspan}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\spn}{\text{span}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\proj}{\text{Proj}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{arg\, max}}
{\text{argmax}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{arg\, min}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\bfx}{\mathbf{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfy}{\mathbf{y}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfl}{\mathbf{\lambda}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\bfm}{\mathbf{\mu}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\calL}{\mathcal{L}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\vw}{\boldsymbol{w}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vx}{\boldsymbol{x}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vxi}{\boldsymbol{\xi}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\valpha}{\boldsymbol{\alpha}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vbeta}{\boldsymbol{\beta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vsigma}{\boldsymbol{\sigma}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\vtheta}{\boldsymbol{\theta}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vd}{\boldsymbol{d}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vs}{\boldsymbol{s}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vt}{\boldsymbol{t}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vh}{\boldsymbol{h}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ve}{\boldsymbol{e}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vf}{\boldsymbol{f}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vg}{\boldsymbol{g}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vz}{\boldsymbol{z}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vk}{\boldsymbol{k}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\va}{\boldsymbol{a}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vb}{\boldsymbol{b}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vv}{\boldsymbol{v}}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\vy}{\boldsymbol{y}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\dom}{\textrm{\textbf{dom} }}
\end_inset


\begin_inset FormulaMacro
\renewcommand{\rank}{\text{\textbf{rank }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\conv}{\textrm{\textbf{conv} }}
\end_inset


\begin_inset FormulaMacro
\newcommand{\relint}{\text{\textbf{relint }}}
\end_inset


\begin_inset FormulaMacro
\newcommand{\aff}{\text{\textbf{aff }}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\hil}{\ch}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\rkhs}{\hil}
\end_inset

 
\begin_inset FormulaMacro
\newcommand{\ber}{\text{Ber}}
\end_inset


\end_layout

\begin_layout Title
Recap for Midterm
\begin_inset Argument 1
status open

\begin_layout Plain Layout
DS-GA 1003 / CSCI-GA 2567
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
optional, use only with long paper titles
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Author
David S.
 Rosenberg 
\end_layout

\begin_layout Date
February 28, 2018
\end_layout

\begin_layout Institute
New York University
\end_layout

\begin_layout Standard
\begin_inset Flex ArticleMode
status open

\begin_layout Plain Layout
Just in article version
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Plots courtesy of Ningshan Zhang.}}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Contents
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\end_deeper
\begin_layout Section
Learning Theory Framework
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Some Formalization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
The Spaces
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout ColumnsTopAligned

\end_layout

\begin_deeper
\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

.3
\backslash
textwidth
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\cx$
\end_inset

: input space
\end_layout

\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

.3
\backslash
textwidth
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\cy$
\end_inset

: outcome space 
\end_layout

\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

.3
\backslash
textwidth
\end_layout

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\ca$
\end_inset

: action space
\end_layout

\end_deeper
\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Prediction Function (or 
\begin_inset Quotes eld
\end_inset

decision function
\begin_inset Quotes erd
\end_inset

)
\end_layout

\end_inset


\end_layout

\begin_layout Block
A 
\series bold
prediction function 
\series default
(or 
\series bold
decision function
\series default
) gets input 
\begin_inset Formula $x\in\cx$
\end_inset

 and produces an action 
\begin_inset Formula $a\in\ca$
\end_inset

 :
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
\begin{matrix}f: & \cx & \rightarrow & \ca\\
\pause & x & \mapsto & f(x)
\end{matrix}
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Loss Function
\end_layout

\end_inset


\end_layout

\begin_layout Block
A 
\series bold
loss function
\series default
 evaluates an action in the context of the outcome 
\begin_inset Formula $y$
\end_inset

.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\[
\begin{matrix}\loss: & \ca\times\cy & \rightarrow & \reals\\
\pause & (a,y) & \mapsto & \loss(a,y)
\end{matrix}
\]

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Risk and the Bayes Prediction Function 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
The 
\series bold
risk
\series default
\emph on
 
\emph default
of a prediction function 
\begin_inset Formula $f:\cx\to\ca$
\end_inset

 is 
\begin_inset Formula 
\[
R(f)=\ex\loss(f(x),y).
\]

\end_inset

In words, it's the 
\series bold
expected loss
\series default
 of 
\begin_inset Formula $f$
\end_inset

 on a new example 
\begin_inset Formula $(x,y)$
\end_inset

 drawn randomly from 
\begin_inset Formula $P_{\cx\times\cy}$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Definition
A 
\series bold
Bayes prediction function
\series default
 
\begin_inset Formula $\minimizer f:\cx\to\ca$
\end_inset

 is a function that achieves the 
\emph on
minimal risk
\emph default
 among all possible functions: 
\begin_inset Formula 
\[
\minimizer f\in\argmin_{f}R(f),
\]

\end_inset

where the minimum is taken over all functions from 
\begin_inset Formula $\cx$
\end_inset

 to 
\begin_inset Formula $\ca$
\end_inset

.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The risk of a Bayes prediction function is called the 
\series bold
Bayes risk
\series default
.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Empirical Risk
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $\cd_{n}=\left((x_{1},y_{1}),\ldots,(x_{n},y_{n})\right)$
\end_inset

 be drawn i.i.d.
 from 
\begin_inset Formula $\cp_{\cx\times\cy}$
\end_inset

.
\end_layout

\begin_layout Itemize
The 
\series bold
empirical risk
\series default
\emph on
 
\emph default
of 
\begin_inset Formula $f:\cx\to\ca$
\end_inset

 with respect to 
\begin_inset Formula $\cd_{n}$
\end_inset

 is
\begin_inset Formula 
\[
\hat{R}_{n}(f)=\frac{1}{n}\sum_{i=1}^{n}\loss(f(x_{i}),y_{i}).
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
A function 
\begin_inset Formula $\hat{f}$
\end_inset

 is an 
\series bold
empirical risk minimizer
\series default
 if
\begin_inset Formula 
\[
\hat{f}\in\argmin_{f}\hat{R}_{n}(f),
\]

\end_inset

where the minimum is taken over all functions.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
But unconstrained ERM can 
\series bold
overfit
\series default
.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Constrained Empirical Risk Minimization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Hypothesis space 
\begin_inset Formula $\cf$
\end_inset

, a set of [prediction] functions mapping 
\begin_inset Formula $\cx\to\ca$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Empirical risk minimizer 
\series default
(ERM) in 
\begin_inset Formula $\cf$
\end_inset

 is 
\begin_inset Formula 
\[
\hat{f}_{n}\in\argmin_{f\in\cf}\frac{1}{n}\sum_{i=1}^{n}\loss(f(x_{i}),y_{i}).
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Risk minimizer 
\series default
in 
\begin_inset Formula $\cf$
\end_inset

 is 
\begin_inset Formula $\minimizer{f_{\cf}}\in\cf$
\end_inset

 , where 
\begin_inset Formula 
\[
\minimizer{f_{\cf}}\in\argmin_{f\in\cf}\ex\loss(f(x),y).
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Error Decomposition
\end_layout

\end_inset

 
\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Block

\end_layout

\begin_deeper
\begin_layout ColumnsCenterAligned

\end_layout

\begin_deeper
\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

.4
\backslash
textwidth
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename est-approx.png
	lyxscale 20
	width 100col%

\end_inset


\end_layout

\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

.4
\backslash
textwidth
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
f^{*}= & \argmin_{f}\ex\ell(f(X),Y)\\
f_{\cf}= & \argmin_{f\in\cf}\ex\ell(f(X),Y))\\
\hat{f}_{n}= & \argmin_{f\in\cf}\frac{1}{n}\sum_{i=1}^{n}\ell(f(x_{i}),y_{i})
\end{align*}

\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Approximation Error 
\series default
(of 
\begin_inset Formula $\cf$
\end_inset

)
\series bold
 
\begin_inset Formula $=\ R(f_{\cf})-R(\minimizer f)$
\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize

\series bold
Estimation error
\series default
 (of 
\begin_inset Formula $\hat{f}_{n}$
\end_inset

 in 
\begin_inset Formula $\cf$
\end_inset

) 
\begin_inset Formula $=\ R(\hat{f}_{n})-R(f_{\cf})$
\end_inset

 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Excess Risk Decomposition for ERM
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The excess risk of the ERM 
\begin_inset Formula $\hat{f}_{n}$
\end_inset

 can be decomposed:
\begin_inset Formula 
\begin{eqnarray*}
\mbox{\textbf{Excess Risk}}(\hat{f}_{n}) & = & \risk(\hat{f}_{n})-\risk(\minimizer f)\\
\pause & = & \underbrace{\risk(\hat{f}_{n})-\risk(f_{\cf})}_{\text{estimation error}}+\underbrace{\risk(f_{\cf})-\risk(\minimizer f)}_{\text{approximation error}}.
\end{eqnarray*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Optimization Error
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
In practice, we don't find the ERM 
\begin_inset Formula $\hat{f}_{n}\in\cf$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Optimization algorithm returns 
\begin_inset Formula $\tilde{f}_{n}\in\cf$
\end_inset

 , which we hope is good enough.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize

\series bold
Optimization error: 
\series default
If 
\begin_inset Formula $\tilde{f}_{n}$
\end_inset

 is the function our optimization method returns, and 
\begin_inset Formula $\hat{f}_{n}$
\end_inset

 is the empirical risk minimizer, then
\begin_inset Formula 
\[
\mbox{Optimization Error }=\;R(\tilde{f}_{n})-R(\hat{f}_{n}).
\]

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Extended decomposition:
\begin_inset Formula 
\begin{align*}
\mbox{\textbf{Excess Risk}}(\tilde{f}_{n})\, & =\,\risk(\tilde{f}_{n})-\risk(\minimizer f)\\
 & =\underbrace{\risk(\tilde{f}_{n})-R(\hat{f}_{n})}_{\text{optimization error}}+\underbrace{\risk(\hat{f}_{n})-\risk(f_{\cf})}_{\text{estimation error}}+\underbrace{\risk(f_{\cf})-\risk(\minimizer f)}_{\text{approximation error}}
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Regularization
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Constrained Empirical Risk Minimization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Constrained ERM (Ivanov regularization)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
For complexity measure 
\begin_inset Formula $\Omega:\cf\to[0,\infty)$
\end_inset

 and fixed 
\begin_inset Formula $r\ge0$
\end_inset

,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\min_{f\in\cf}\; & \frac{1}{n}\sum_{i=1}^{n}\ell(f(x_{i}),y_{i})\\
\mbox{s.t.}\; & \Omega(f)\le r
\end{align*}

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Choose 
\begin_inset Formula $r$
\end_inset

 using validation data or cross-validation.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Each 
\begin_inset Formula $r$
\end_inset

 corresponds to a different hypothesis spaces.
 Could also write:
\begin_inset Formula 
\[
\min_{f\in\cf_{r}}\frac{1}{n}\sum_{i=1}^{n}\ell(f(x_{i}),y_{i})
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Penalized Empirical Risk Minimization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Penalized ERM (Tikhonov regularization)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
For complexity measure 
\begin_inset Formula $\Omega:\cf\to[0,\infty)$
\end_inset

 and fixed 
\begin_inset Formula $\lambda\ge0$
\end_inset

,
\begin_inset Formula 
\begin{align*}
\min_{f\in\cf} & \frac{1}{n}\sum_{i=1}^{n}\ell(f(x_{i}),y_{i})+\lambda\Omega(f)
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Choose 
\begin_inset Formula $\lambda$
\end_inset

 using validation data or cross-validation.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
(Ridge regression in homework is of this form.)
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Ridge Regression: Workhorse of Modern Data Science
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Ridge Regression (Tikhonov Form)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The ridge regression solution for regularization parameter 
\begin_inset Formula $\lambda\ge0$
\end_inset

 is
\begin_inset Formula 
\[
\hat{w}=\argmin_{w\in\reals^{d}}\frac{1}{n}\sum_{i=1}^{n}\left\{ w^{T}x_{i}-y_{i}\right\} ^{2}+\lambda\|w\|_{2}^{2},
\]

\end_inset

where 
\begin_inset Formula $\|w\|_{2}^{2}=w_{1}^{2}+\cdots+w_{d}^{2}$
\end_inset

 is the square of the 
\begin_inset Formula $\ell_{2}$
\end_inset

-norm.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Ridge Regression (Ivanov Form)
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Standard
The ridge regression solution for complexity parameter 
\begin_inset Formula $r\ge0$
\end_inset

 is
\begin_inset Formula 
\[
\hat{w}=\argmin_{\|w\|_{2}^{2}\le r^{2}}\frac{1}{n}\sum_{i=1}^{n}\left\{ w^{T}x_{i}-y_{i}\right\} ^{2}.
\]

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Lasso Regression: Workhorse (2) of Modern Data Science
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Lasso Regression (Tikhonov Form)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
The lasso regression solution for regularization parameter 
\begin_inset Formula $\lambda\ge0$
\end_inset

 is
\begin_inset Formula 
\[
\hat{w}=\argmin_{w\in\reals^{d}}\frac{1}{n}\sum_{i=1}^{n}\left\{ w^{T}x_{i}-y_{i}\right\} ^{2}+\lambda\|w\|_{1},
\]

\end_inset

where 
\begin_inset Formula $\|w\|_{1}=\left|w_{1}\right|+\cdots+\left|w_{d}\right|$
\end_inset

 is the 
\begin_inset Formula $\ell_{1}$
\end_inset

-norm.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Lasso Regression (Ivanov Form)
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Standard
The lasso regression solution for complexity parameter 
\begin_inset Formula $r\ge0$
\end_inset

 is
\begin_inset Formula 
\[
\hat{w}=\argmin_{\|w\|_{1}\le r}\frac{1}{n}\sum_{i=1}^{n}\left\{ w^{T}x_{i}-y_{i}\right\} ^{2}.
\]

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Ridge vs.
 Lasso: Regularization Paths
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Modified from Hastie, Tibshirani, and Wainwright's 
\backslash
emph{Statistical Learning with Sparsity}, Fig 2.1.
 About predicting crime in 50 US cities.}}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename lasso-ridge-paths-side-by-side.png
	lyxscale 30
	height 80theight%

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Linearly Dependent Features: Take Away
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
For identical features
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\ell_{1}$
\end_inset

 regularization spreads weight arbitrarily (all weights same sign)
\end_layout

\begin_layout Itemize
\begin_inset Formula $\ell_{2}$
\end_inset

 regularization spreads weight evenly 
\end_layout

\end_deeper
\begin_layout Itemize
Linearly related features
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\ell_{1}$
\end_inset

 regularization chooses variable with larger scale, 0 weight to others
\end_layout

\begin_layout Itemize
\begin_inset Formula $\ell_{2}$
\end_inset

 prefers variables with larger scale – spreads weight proportional to scale
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Correlated Features, 
\begin_inset Formula $\ell_{1}$
\end_inset

 Regularization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename L1Corr.png
	lyxscale 20
	height 40theight%

\end_inset


\begin_inset space \hspace{}
\length 20text%
\end_inset


\begin_inset Graphics
	filename L1Corr2.png
	lyxscale 20
	height 40theight%

\end_inset


\end_layout

\begin_layout Itemize
Intersection could be anywhere on the top right edge.
 
\end_layout

\begin_layout Itemize
Minor perturbations (in data) can drastically change intersection point
 – very unstable solution.
\end_layout

\begin_layout Itemize
Makes division of weight among highly correlated features (of same scale)
 seem arbitrary.
\end_layout

\begin_deeper
\begin_layout Itemize
If 
\begin_inset Formula $x_{1}\approx2x_{2}$
\end_inset

, ellipse changes orientation and we hit a corner.
 (Which one?)
\begin_inset Note Note
status open

\begin_layout Plain Layout
Plugging into 
\begin_inset Formula $w_{1}x_{2}+w_{2}x_{2}=k$
\end_inset

, we get 
\begin_inset Formula $\left(2w_{1}+w_{2}\right)x_{2}=k$
\end_inset

, when 
\begin_inset Formula $x_{1}=2x_{2}$
\end_inset

, so we get equivalent predictors for all parameter values on the line 
\begin_inset Formula $2w_{1}+w_{2}=c$
\end_inset

, which has slope negative 
\begin_inset Formula $2$
\end_inset

, so we'll hit the corner on the 
\begin_inset Formula $w_{1}$
\end_inset

 axis, which corresponds to 
\begin_inset Formula $w_{2}=0$
\end_inset

, which makes sense, since 
\begin_inset Formula $x_{2}$
\end_inset

 has smaller scale.
\end_layout

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Elastic Net
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The 
\series bold
elastic net
\series default
 combines lasso and ridge penalties:
\begin_inset Formula 
\[
\hat{w}=\argmin_{w\in\reals^{d}}\frac{1}{n}\sum_{i=1}^{n}\left\{ w^{T}x_{i}-y_{i}\right\} ^{2}+\lambda_{1}\|w\|_{1}+\lambda_{2}\|w\|_{2}^{2}
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
We expect correlated random variables to have similar coefficients.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Highly Correlated Features, Elastic Net Constraint
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename EnetCorr.png
	lyxscale 18
	height 50theight%

\end_inset


\end_layout

\begin_layout Itemize
Elastic net solution is closer to 
\begin_inset Formula $w_{2}=w_{1}$
\end_inset

 line, despite high correlation.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Elastic Net Results on Model
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename L1regpaths-correlated-vars.png
	lyxscale 20
	height 60theight%

\end_inset


\begin_inset Graphics
	filename elasticNetregpaths-correlated-vars.png
	lyxscale 20
	height 60theight%

\end_inset


\end_layout

\begin_layout Itemize
Lasso on left; Elastic net on right.
\end_layout

\begin_layout Itemize
Ratio of 
\begin_inset Formula $\ell_{2}$
\end_inset

 to 
\begin_inset Formula $\ell_{1}$
\end_inset

 regularization roughly 
\begin_inset Formula $2:1$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Elastic Net - 
\begin_inset Quotes eld
\end_inset

Sparse Regions
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
let
\backslash
thefootnote
\backslash
relax
\backslash
footnotetext{
\backslash
tiny{Fig from 
\backslash
href{https://arxiv.org/abs/1411.3230}{Mairal et al.'s Sparse Modeling for Image
 and Vision Processing} Fig 1.9}}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename projections-to-ElasticNetBall.png
	lyxscale 40
	height 50theight%

\end_inset


\begin_inset VSpace -0.3cm
\end_inset


\end_layout

\begin_layout Itemize
Suppose design matrix 
\begin_inset Formula $X$
\end_inset

 is orthogonal, so 
\begin_inset Formula $X^{T}X=I$
\end_inset

, and contours are circles (and features uncorrelated)
\end_layout

\begin_layout Itemize
Then OLS solution in green or red regions implies elastic-net constrained
 solution will be at corner
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Elastic Net Summary
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
With uncorrelated features, we can get sparsity.
\end_layout

\begin_layout Itemize
Among correlated features (same scale), we spread weight more evenly.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Finding Lasso Solution
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Many options.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Convert to quadratic program using positive/negative parts
\begin_inset Formula 
\begin{align*}
\min_{w^{+},w^{-}}\quad & \sum_{i=1}^{n}\left(\left(w^{+}-w^{-}\right)^{T}x_{i}-y_{i}\right)^{2}+\lambda1^{T}\left(w^{+}+w^{-}\right)\\
\mbox{subject to}\quad & w_{i}^{+}\ge0\mbox{ for all }i\text{\qquad}w_{i}^{-}\ge0\mbox{ for all }i,
\end{align*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Coordinate descent
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Lasso has closed form solution for coordinate minimizers!
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\end_deeper
\begin_layout Itemize
Subgradient descent
\end_layout

\end_deeper
\begin_layout Section
Optimization
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent for Empirical Risk and Averages
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suppose we have a hypothesis space of functions 
\begin_inset Formula $\cf=\left\{ f_{w}:\cx\to\ca\mid w\in\reals^{d}\right\} $
\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Parameterized by 
\begin_inset Formula $w\in\reals^{d}$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
ERM is to find 
\begin_inset Formula $w$
\end_inset

 minimizing
\begin_inset Formula 
\[
\hat{R}_{n}(w)=\frac{1}{n}\sum_{i=1}^{n}\loss(f_{w}(x_{i}),y_{i})
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Suppose 
\begin_inset Formula $\loss(f_{w}(x_{i}),y_{i})$
\end_inset

 is differentiable as a function of 
\begin_inset Formula $w$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Then we can do gradient descent on 
\begin_inset Formula $\hat{R}_{n}(w)$
\end_inset

...
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Gradient Descent: How does it scale with 
\begin_inset Formula $n$
\end_inset

?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
At every iteration, we compute the gradient at current 
\begin_inset Formula $w$
\end_inset

:
\begin_inset Formula 
\[
\del\hat{R}_{n}(w)=\frac{1}{n}\sum_{i=1}^{n}\del_{w}\ell(f_{w}(x_{i}),y_{i})
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
We have to touch all 
\begin_inset Formula $n$
\end_inset

 training points to take a single step.
 [
\begin_inset Formula $O(n)$
\end_inset

]
\begin_inset Note Note
status collapsed

\begin_layout Pause

\end_layout

\begin_layout Itemize
A method that looks at all training points before each step is called a
 
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
batch optimization
\series default
 method.
 
\end_layout

\begin_layout Itemize
So far we've presented 
\series bold

\begin_inset Quotes eld
\end_inset

batch gradient descent
\begin_inset Quotes erd
\end_inset

.
\end_layout

\end_deeper
\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
What if we just use an estimate of the gradient?
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Minibatch Gradient
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The 
\series bold
full gradient
\series default
 is
\begin_inset Formula 
\[
\del\hat{R}_{n}(w)=\frac{1}{n}\sum_{i=1}^{n}\del_{w}\ell(f_{w}(x_{i}),y_{i})
\]

\end_inset


\end_layout

\begin_layout Itemize
It's an average over the 
\series bold
full batch
\series default
 of data 
\begin_inset Formula $\cd_{n}=\left\{ (x_{1},y_{1}),\ldots,(x_{n},y_{n})\right\} $
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Let's take a random subsample of size 
\begin_inset Formula $N$
\end_inset

 (called a 
\series bold
minibatch
\series default
):
\begin_inset Formula 
\[
(x_{m_{1}},y_{m_{1}}),\ldots,(x_{m_{N}},y_{m_{N}})
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The 
\series bold
minibatch gradient is
\begin_inset Formula 
\[
\del\hat{R}_{N}(w)=\frac{1}{N}\sum_{i=1}^{N}\del_{w}\ell(f_{w}(x_{m_{i}}),y_{m_{i}})
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Minibatch gradient is an unbiased estimate of full-batch gradient: 
\begin_inset Formula $\ex\left[\del\hat{R}_{N}(w)\right]=\del\hat{R}_{n}(w)$
\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
How big should minibatch be?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Tradeoffs of minibatch size:
\end_layout

\begin_deeper
\begin_layout Itemize
Bigger 
\begin_inset Formula $N$
\end_inset

 
\begin_inset Formula $\implies$
\end_inset

Better estimate of gradient, but slower (more data to touch)
\end_layout

\begin_layout Itemize
Smaller 
\begin_inset Formula $N$
\end_inset

 
\begin_inset Formula $\implies$
\end_inset

Worse estimate of gradient, but can be quite fast 
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Even 
\begin_inset Formula $N=1$
\end_inset

 works, it's traditionally called 
\series bold
stochastic gradient descent
\series default
 (SGD).
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Quality of minibatch estimate depends on
\end_layout

\begin_deeper
\begin_layout Itemize
size of minibatch
\end_layout

\begin_layout Itemize
but is 
\series bold
independent 
\series default
of full dataset size 
\begin_inset Formula $n$
\end_inset


\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Discussed in Concept Check question.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Descent Directions
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
A step direction is a 
\series bold
descent direction
\series default
 if, for small enough step size, the objective function value always decreases.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Negative gradient is a descent direction.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
A negative subgradient is 
\series bold
not
\series default
 a descent direction.
 But always 
\series bold
takes you closer to a minimizer
\series default
.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Negative stochastic or minibatch gradient direction is 
\series bold
not
\series default
 a descent direction.
 But we have convergence theorems.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Negative stochastic subgradient step direction is 
\series bold
not
\series default
 a descent direction.
 But we have convergence theorems (not discussed in class).
\end_layout

\end_deeper
\begin_layout Section
Classification
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Score Function
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Action space 
\begin_inset Formula $\ca=\reals\qquad$
\end_inset

 Output space 
\begin_inset Formula $\cy=\left\{ -1,1\right\} $
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Real-valued prediction function
\series default
 
\begin_inset Formula $f:\cx\to\reals$
\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Definition
The value 
\begin_inset Formula $f(x)$
\end_inset

 is called the 
\series bold
score
\series default
 for the input 
\begin_inset Formula $x$
\end_inset

.
 
\end_layout

\begin_layout Itemize
In this context, 
\begin_inset Formula $f$
\end_inset

 may be called a 
\series bold
score function
\series default
.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Intuitively, magnitude of the score represents the 
\series bold
confidence of our prediction
\series default
.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Margin
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Definition
The 
\series bold
margin
\series default
 (or 
\series bold
functional margin
\series default
)
\series bold
 
\series default
for predicted score 
\begin_inset Formula $\hat{y}$
\end_inset

 and true class 
\begin_inset Formula $y\in\left\{ -1,1\right\} $
\end_inset

 is 
\begin_inset Formula $y\hat{y}$
\end_inset

.
 
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The margin often looks like 
\begin_inset Formula $yf(x)$
\end_inset

, where 
\begin_inset Formula $f(x)$
\end_inset

 is our score function.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
The margin is a measure of how 
\series bold
correct
\series default
 we are.
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $y$
\end_inset

 and 
\begin_inset Formula $\hat{y}$
\end_inset

 are the same sign, prediction is 
\series bold
correct
\series default
 and margin is 
\series bold
positive
\series default
.
\end_layout

\begin_layout Itemize
If 
\begin_inset Formula $y$
\end_inset

 and 
\begin_inset Formula $\hat{y}$
\end_inset

 have different sign, prediction is 
\series bold
incorrect
\series default
 and margin is 
\series bold
negative
\series default
.
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
We want to 
\series bold
maximize the margin
\series default
.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Classification Losses
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Logistic/Log loss: 
\begin_inset Formula $\loss_{\text{Logistic}}=\log\left(1+e^{-m}\right)$
\end_inset


\end_layout

\begin_layout Standard
\align center
\begin_inset Graphics
	filename loss.Zero_One.Hinge.Logistic.png
	lyxscale 30
	height 70theight%

\end_inset


\end_layout

\begin_layout Standard
Logistic loss is differentiable.
 Logistic loss always wants more margin (loss never 0).
\begin_inset Note Note
status open

\begin_layout Plain Layout
How many support vectors?
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Support Vector Machine 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Hypothesis space 
\begin_inset Formula $\cf=\left\{ f(x)=w^{T}x+b\mid w\in\reals^{d},\,b\in\reals\right\} $
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
\begin_inset Formula $\ell_{2}$
\end_inset

 regularization (Tikhonov style)
\end_layout

\begin_layout Itemize
Loss 
\begin_inset Formula $\ell(m)=\max\left\{ 1-m,0\right\} $
\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
The SVM prediction function is the solution to
\begin_inset Formula 
\[
\min_{w\in\reals^{d},b\in\reals}\frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\max\left(0,1-y_{i}\left[w^{T}x_{i}+b\right]\right).
\]

\end_inset

 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
SVM as a Quadratic Program
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The SVM optimization problem is equivalent to
\begin_inset Formula 
\begin{eqnarray*}
\textrm{minimize} &  & \frac{1}{2}||w||^{2}+\frac{c}{n}\sum_{i=1}^{n}\xi_{i}\\
\textrm{subject to} &  & -\xi_{i}\le0\;\mbox{for }i=1,\ldots,n\\
 &  & \left(1-y_{i}\left[w^{T}x_{i}+b\right]\right)-\xi_{i}\le0\;\mbox{for }i=1,\ldots,n
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Differentiable objective function
\end_layout

\begin_layout Itemize
\begin_inset Formula $n+d+1$
\end_inset

 unknowns and 
\begin_inset Formula $2n$
\end_inset

 affine constraints.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
A quadratic program that can be solved by any off-the-shelf QP solver.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
The Representer Theorem and Kernelization
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
General Objective Function for Linear Hypothesis Space (Details)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Generalized objective
\series default
: 
\begin_inset Formula 
\[
\min_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right),
\]

\end_inset

where
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $w,x_{1},\ldots,x_{n}\in\ch$
\end_inset

 for some Hilbert space 
\begin_inset Formula $\ch$
\end_inset

.
 (We typically have 
\begin_inset Formula $\ch=\reals^{d}.)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\|\cdot\|$
\end_inset

 is the norm corresponding to the inner product of 
\begin_inset Formula $\ch$
\end_inset

.
 (i.e.
 
\begin_inset Formula $\|w\|=\sqrt{\left\langle w,w\right\rangle }$
\end_inset

) 
\end_layout

\begin_layout Itemize
\begin_inset Formula $R:[0,\infty)\to\reals$
\end_inset

 is nondecreasing (
\series bold
Regularization term
\series default
), and
\end_layout

\begin_layout Itemize
\begin_inset Formula $L:\reals^{n}\to\reals$
\end_inset

 is arbitrary (
\series bold
Loss term
\series default
).
\end_layout

\end_deeper
\begin_layout Pause

\end_layout

\begin_layout Itemize
Ridge regression and SVM are of this form.
 
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
What if we use lasso regression? 
\begin_inset Formula $\pause$
\end_inset

No! 
\begin_inset Formula $\ell_{1}$
\end_inset

 norm does not correspond to an inner product.
 
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status collapsed

\begin_layout Plain Layout
The Representer Theorem
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
Let 
\begin_inset Formula $J(w)=R\left(\|w\|\right)+L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right)$
\end_inset

 under conditions described above.
\end_layout

\begin_layout Theorem
\begin_inset ERT
status open

\begin_layout Plain Layout

[Representer Theorem]
\end_layout

\end_inset

If 
\begin_inset Formula $J(w)$
\end_inset

 has a minimizer, then it 
\series bold
has a minimizer of the form
\series default
 
\begin_inset Formula 
\[
w^{*}=\sum_{i=1}^{n}\alpha_{i}x_{i}.
\]

\end_inset

If 
\begin_inset Formula $R$
\end_inset

 is strictly increasing, then all minimizers have this form.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Standard
Basic idea of proof:
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $M=\linspan\left(x_{1},\ldots,x_{n}\right)$
\end_inset

.
 [the 
\series bold

\begin_inset Quotes eld
\end_inset

span of the data
\series default

\begin_inset Quotes erd
\end_inset

]
\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $w=\proj_{M}w^{*}$
\end_inset

, for some minimizer 
\begin_inset Formula $w^{*}$
\end_inset

 of 
\begin_inset Formula $J(w)$
\end_inset

.
\end_layout

\begin_layout Itemize
Then 
\begin_inset Formula $\left\langle w,x_{i}\right\rangle =\left\langle w^{*},x_{i}\right\rangle $
\end_inset

, so loss part doesn't change.
\end_layout

\begin_layout Itemize
\begin_inset Formula $\|w\|\le\|w^{*}\|$
\end_inset

, since projection reduces norm.
 So regularization piece never increases.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Reparametrization with Representer Theorem
\end_layout

\end_inset

 
\end_layout

\begin_deeper
\begin_layout Itemize
Original plan: 
\end_layout

\begin_deeper
\begin_layout Itemize
Find 
\begin_inset Formula $w^{*}\in\argmin_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right)$
\end_inset


\end_layout

\begin_layout Itemize
Predict with 
\begin_inset Formula $\hat{f}(x)=\left\langle w^{*},x\right\rangle $
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Plugging in result of representer theorem, it's equivalent to
\end_layout

\begin_deeper
\begin_layout Itemize
Find 
\begin_inset Formula $\alpha^{*}\in\argmin_{\alpha\in\reals^{n}}R\left(\sqrt{\alpha^{T}K\alpha}\right)+L\left(K\alpha\right)$
\end_inset


\end_layout

\begin_layout Itemize
Predict with 
\begin_inset Formula $\hat{f}(x)=k_{x}^{T}\alpha^{*}\pause$
\end_inset

, where
\begin_inset Formula 
\[
K=\begin{pmatrix}\left\langle x_{1},x_{1}\right\rangle  & \cdots & \left\langle x_{1},x_{n}\right\rangle \\
\vdots & \ddots & \cdots\\
\left\langle x_{n},x_{1}\right\rangle  & \cdots & \left\langle x_{n},x_{n}\right\rangle 
\end{pmatrix}\qquad\text{and}\qquad k_{x}=\begin{pmatrix}\left\langle x_{1},x\right\rangle \\
\vdots\\
\left\langle x_{n},x\right\rangle 
\end{pmatrix}
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Every element 
\begin_inset Formula $x\in\ch$
\end_inset

 occurs inside an inner products with a training input 
\begin_inset Formula $x_{i}\in\ch$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelization
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Definition
A method is 
\series bold
kernelized 
\series default
if every feature vector 
\begin_inset Formula $\psi(x)$
\end_inset

 only appears inside an inner product with another feature vector 
\begin_inset Formula $\psi(x')$
\end_inset

.
 This applies to both the optimization problem and the prediction function.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Here we are using 
\begin_inset Formula $\psi(x)=x$
\end_inset

.
 Thus finding 
\begin_inset Formula 
\[
\alpha^{*}\in\argmin_{\alpha\in\reals^{n}}R\left(\sqrt{\alpha^{T}K\alpha}\right)+L\left(K\alpha\right)
\]

\end_inset

 and making predictions with 
\begin_inset Formula $\hat{f}(x)=k_{x}^{T}\alpha^{*}$
\end_inset

 is a 
\series bold
kernelization
\series default
 of finding
\begin_inset Formula 
\[
w^{*}\in\argmin_{w\in\ch}R\left(\|w\|\right)+L\left(\left\langle w,x_{1}\right\rangle ,\ldots,\left\langle w,x_{n}\right\rangle \right)
\]

\end_inset

 and making predictions with 
\begin_inset Formula $\hat{f}(x)=\left\langle w^{*},x\right\rangle $
\end_inset

.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Once we have kernelized:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\alpha^{*}\in\argmin_{\alpha\in\reals^{n}}R\left(\sqrt{\alpha^{T}K\alpha}\right)+L\left(K\alpha\right)$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\hat{f}(x)=k_{x}^{T}\alpha^{*}$
\end_inset

 
\end_layout

\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
We can do the 
\begin_inset Quotes eld
\end_inset

kernel trick
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Replace each 
\begin_inset Formula $\left\langle x,x'\right\rangle $
\end_inset

 by 
\begin_inset Formula $k(x,x')$
\end_inset

, for any kernel function 
\begin_inset Formula $k$
\end_inset

, where 
\begin_inset Formula $k(x,x')=\left\langle \psi(x),\psi(x')\right\rangle $
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Predictions 
\begin_inset Formula 
\[
\hat{f}(x)=\sum_{i=1}^{n}\alpha_{i}^{*}k(x_{i},x)
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
The Kernel Function: Why do we need this?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Feature map
\series default
: 
\begin_inset Formula $\psi:\cx\to\ch$
\end_inset


\end_layout

\begin_layout Itemize
The 
\series bold
kernel function
\series default
 corresponding to 
\begin_inset Formula $\psi$
\end_inset

 is 
\begin_inset Formula 
\[
k(x,x')=\left\langle \psi(x),\psi(x')\right\rangle .
\]

\end_inset


\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
Why introduce this new notation 
\begin_inset Formula $k(x,x')$
\end_inset

?
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
We can often evaluate 
\begin_inset Formula $k(x,x')$
\end_inset

 without explicitly computing 
\begin_inset Formula $\psi(x)$
\end_inset

 and 
\begin_inset Formula $\psi(x')$
\end_inset

.
\end_layout

\begin_layout Pause

\end_layout

\begin_layout Itemize
For large feature spaces, can be much faster.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Kernelized SVM (From Lagrangian Duality) 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Kernelized SVM from computing the Lagrangian Dual Problem:
\begin_inset Formula 
\begin{eqnarray*}
\max_{\alpha\in\reals^{n}} &  & \sum_{i=1}^{n}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{j}^{T}x_{i}\\
\mbox{s.t.} &  & \sum_{i=1}^{n}\alpha_{i}y_{i}=0\\
 & \quad & \alpha_{i}\in\left[0,\frac{c}{n}\right]\;i=1,\ldots,n.
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
If 
\begin_inset Formula $\alpha^{*}$
\end_inset

 is an optimal value, then
\begin_inset Formula 
\[
w^{*}=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}x_{i}\qquad\text{and}\qquad\hat{f}(x)=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}x_{i}^{T}x.
\]

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
Note that the prediction function is also kernelized.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Sparsity in the Data from Complementary Slackness
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Kernelized predictions given by
\begin_inset Formula 
\[
\hat{f}(x)=\sum_{i=1}^{n}\alpha_{i}^{*}y_{i}x_{i}^{T}x.
\]

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Seems to need all training inputs 
\begin_inset Formula $x_{1},\ldots,x_{n}$
\end_inset

 to make a prediction on a new 
\begin_inset Formula $x$
\end_inset

.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
By a Lagrangian duality analysis (specifically from complementary slackness),
 we find 
\begin_inset Formula 
\begin{eqnarray*}
y_{i}\hat{f}(x_{i})<1 & \implies & \alpha_{i}^{*}=\frac{c}{n}\\
y_{i}\hat{f}(x_{i})=1 & \implies & \alpha_{i}^{*}\in\left[0,\frac{c}{n}\right]\\
y_{i}\hat{f}(x_{i})>1 & \implies & \alpha_{i}^{*}=0
\end{eqnarray*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
So we can leave out any 
\begin_inset Formula $x_{i}$
\end_inset

 
\begin_inset Quotes eld
\end_inset

on the good side of the margin
\begin_inset Quotes erd
\end_inset

 (
\begin_inset Formula $y_{i}\hat{f}(x_{i})>1$
\end_inset

).
\end_layout

\begin_deeper
\begin_layout Pause

\end_layout

\end_deeper
\begin_layout Itemize
\begin_inset Formula $x_{i}$
\end_inset

's that we must keep, because 
\begin_inset Formula $\alpha_{i}^{*}\neq0$
\end_inset

, are called 
\series bold
support vectors
\series default
.
 
\end_layout

\end_deeper
\end_body
\end_document
