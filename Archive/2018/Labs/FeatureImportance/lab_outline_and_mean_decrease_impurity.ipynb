{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance outline\n",
    "\n",
    "### Feature importance\n",
    "- Introduction and motivation for discussing feature importance:\n",
    "    - Define feature importance (using a fuzzy definition -- rank features by how much they \"*contribute*\" to the prediction function).\n",
    "    - Feature importance measures are frequently reported, in part because many packages make them easy to compute/visualize.\n",
    "        - Show google image search results\n",
    "    - They can be potentially misleading. Important to understand potential pitfalls.\n",
    "    - Utility of feature importance:\n",
    "        - Feature selection -- use votes example from original Breiman Random Forests paper.\n",
    "        - Sanity checking models/detecting leakage (TODO: see if I can find an example where feature importance show some feature unexpectedly popping since due to leakage).\n",
    "- Framing feature importance methods:\n",
    "    - Divide into two classes:\n",
    "        - Methods that exploit the structure of the prediction function or learning algorithm (call these function-specific).\n",
    "        - Methods that are agnostic (i.e. can be applied to any black box prediction function).\n",
    "- Function-specific methods:\n",
    "    - Mean decrease impurity:\n",
    "        - Frame running example:\n",
    "            - Introduce the iris dataset and classification problem.\n",
    "            - graphviz some shallow tree on the iris dataset (see below).\n",
    "            - **Ask students** which features seem \"important\" as a turn and talk. Have some subset share answers without explanation.\n",
    "        - Introduce MDE:\n",
    "        $$Imp(X_m) = \\frac{1}{N_T}\\sum_T\\sum_{t \\in T:v(s_t)=X_m}p(t)\\Delta i(s_t,t)$$\n",
    "        - Show from scratch implimentation (see lab_outline_and_mean_decrease_impurity.ipynb).\n",
    "        - Show correspondance with sklearn's implementation.\n",
    "        - **Ask students** what other classifiers we could extend this too (Random Forests/GBMs) and how (averaging).\n",
    "        - Show RF importance for the iris problem.\n",
    "        - Say there are variants:\n",
    "            - Example: CART implementation exposes a knob to add weight for use of a feature as a surrogate split.\n",
    "    - Absolute coefficients in a linear model:\n",
    "        - **Ask students** how we could determine feature importance in a linear model.\n",
    "        - Show some example (TODO) of extracting absolute $\\beta$s from a model.\n",
    "        - **Ask students**: How would this interact with preprocessing?\n",
    "- Model agnostic methods:\n",
    "    - Frame the general problem for global feature importance:\n",
    "        - Another framing of feature importance -- we are trying to identify how much each feature independently contributes to risk reduction in the learned model.\n",
    "        - **Ask students**: How could we do this for an arbitrary black box prediction function?\n",
    "        - **Hint (fragment slide)**: Imagine you were tasked with constructing a synthetic dataset with an *unimportant* feature -- how could you construct it? Given this insight, what operation could be applied to a feature that would be expected to have (a) no impact on empirical risk for unimportant features, and (b) decrease performance for important featuers?\n",
    "    - Permutation importance:\n",
    "        - Give Breimans original OOB permutation feature importance.\n",
    "        - Say that the OOB version exploits bootstrapping in RFs, but can just use a validation set.\n",
    "        - Show example (using ELI5 implementation) over the iris problem.\n",
    "        - **Ask students**: This method permute features to get a measure of feature importance. Let's extend this concept a bit -- how could permuting the target be useful in interogating the performance of a model?\n",
    "        - http://jmlr.csail.mit.edu/papers/volume11/ojala10a/ojala10a.pdf\n",
    "        - For fun show this using permutation_test_score for the iris data.\n",
    "- Pitfalls:\n",
    "    - Correlated features\n",
    "        - Show the GBM feature importance for duplicated or highly correlated features (TODO: think about which is better for this purpose) (from lab_outline_and_mean_decrease_impurity.ipynb).\n",
    "        - **Ask students**: Interpret this plot. What can we say about the features/model?\n",
    "        - Show same plot for enet, and ask same question -- hopefully students will recall that (at least in the context of linear models) our conversations about feature correlation.\n",
    "        - Show the elastnic net bound result as review, and primer to think about correlation issues.\n",
    "        - Show correlation matrix/matrices (duplicated and/or highly correlated), and known data generating process.\n",
    "        - **Ask students**: Given what we know about (i) linear methods, and (ii) trees, what does this show us about potential pitfalls in interpreting feature importance plots?\n",
    "        - **Ask students**: How would this impact SelectFromModel feature selection procedures (ala sklearn) which select the most important features from a model based on either absolute coefficients or feature importance scores? Why might we want to use RFE?\n",
    "\n",
    "### PDPs\n",
    "- Motivation and introduction to partial dependence plots\n",
    "    - Imagine we have a subset of features we think are important.\n",
    "        - This could be based on \"feature importance\" scores or coefficients, or other reasons (i.e. prior knowledge/research questions/etc.).\n",
    "    - We may want to dig deeper to explain the relationship between our predictions and these features.\n",
    "        - **Ask students**: Why? If we have MDI feature importance scores in hand, what do we not know?\n",
    "            - Directionality:\n",
    "                - Note in reality we probably have complex, non-monotonic relationships\n",
    "                - However even if we have a monotonic relationship, or even linear, our MDI wouldn't give any indication on the direction -- obviously weights in a linear model would.\n",
    "    - Partial dependence plots let us dig deeper and visualize the dependence of our prediction function (i.e. predict/predict_proba) on one or two of these features. Can't easily go higher-dimensional (Why not? Can't visualize).\n",
    "    - Show basic math (marginal and empirical estimate -- see https://arxiv.org/pdf/1309.6392.pdf for useful framing).\n",
    "\n",
    "- PDP examples:\n",
    "    - Show sklearn example (see partial_dependence.ipynb).\n",
    "    - **Ask students**: What does this plot show? How do we interpret it? \n",
    "- PDP pitfalls:\n",
    "    - Show the flat partial dependence plot from (https://arxiv.org/pdf/1309.6392.pdf) reproduced in partial_dependence.ipynb.\n",
    "    - **Ask students**: What does this plot show?\n",
    "    - Follow up with big reveal:\n",
    "        - Show data generating process and X plot.\n",
    "    - **Ask students**: What does this teach us? When might PDPs fail?\n",
    "    - Share \"not too strong\" quote from Friedman's original work (see partial_dependence.ipynb).\n",
    "    \n",
    "### Additional material\n",
    "- If 100 minutes allows (what is your opinion, David?), the next steps would be:\n",
    "    - Generate the ICE plot for the X data (figure 2 in https://arxiv.org/pdf/1309.6392.pdf)\n",
    "    - Take this  as a launching point for point out that our progression has been from:\n",
    "        - Global measures (feature importance) that don't necessarily describe structure (even directionality) of a relationship between a feature and target.\n",
    "        - Marginalized measures showing average relationship between one or two features and the target.\n",
    "        - Data-instance specific feature importance plots showing the importance of a feature, for each data instance, fixing all other feature values.\n",
    "    - We can follow this thread it's conclusion -- attempting to develop model explanations that show the relative contribution of each feature to predictions locally (i.e. in the neighborhood of a specific instance):\n",
    "        - LIME paper (https://arxiv.org/abs/1602.04938) as one example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature importance outline for trees\n",
    "\n",
    "This notebook outlines some concepts and code for mean decrease impurity and permutation feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "def hide_code_in_slideshow():\n",
    "    import os\n",
    "    uid = os.urandom(8).encode(\"hex\")\n",
    "    html = \"\"\"<div id=\"%s\"></div>\n",
    "    <script type=\"text/javascript\">\n",
    "        $(function(){\n",
    "            var p = $(\"#%s\");\n",
    "            if (p.length==0) return;\n",
    "\n",
    "            while (!p.hasClass(\"cell\")) {\n",
    "                p=p.parent();\n",
    "\n",
    "                if (p.prop(\"tagName\") ==\"body\") return;\n",
    "            }\n",
    "            var cell = p;\n",
    "            cell.find(\".input\").addClass(\"hide-in-slideshow\")\n",
    "        });\n",
    "    </script>\"\"\" % (uid, uid)\n",
    "    display.display_html(html, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       " .container.slides .celltoolbar, .container.slides .hide-in-slideshow {\n",
       "    display: None ! important;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    " .container.slides .celltoolbar, .container.slides .hide-in-slideshow {\n",
    "    display: None ! important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lab outline (for this notebook)\n",
    "\n",
    "### 1. What is feature imortance and why do we want to explore it?\n",
    "- Intepret/interogate models:\n",
    "    - This is not causal, but can still be useful.\n",
    "    - As a decidely non-causal example, perhaps looking at feature importance will give you some insight into leakage if a feature pops unexpectedly (you always want to sanity check your prediction functions).\n",
    "    - Alternatively, it might help you with feature selection (i.e. `SelectFromModel`).\n",
    "- Feature selection:\n",
    "    - Show permutation example from original Random Forest paper on voting patterns -- using a single feature OOB performance is almost as good.\n",
    "    - Danger!!!\n",
    "        - (From eli5 docs) Permutation importance should be used for feature selection with care (like many other feature importance measures). For example, if several features are correlated, and the estimator uses them all equally, permutation importance can be low for all of these features: dropping one of the features may not affect the result, as estimator still has an access to the same information from other features. So if features are dropped based on importance threshold, such correlated features could be dropped all at the same time, regardless of their usefulness. RFE and alike methods (as opposed to single-stage feature selection) can help with this problem to an extent.\n",
    "        - Show this with ElasticNet, since there we can revisit our theorem on correlated features.\n",
    "- We can think about two general sets of feature importance measures:\n",
    "    - Model-based feature importance measures. These are importance measures that we can compute based on the structure of the prediction function/and-or learning algorithm. We have these for tree-based models and linear models (weights/mean decrease impurity in a tree -- shown in this notebook).\n",
    "    - Generalized feature imortance framework -- can compute a \"feature importance\" measure for an arbitrary (black box) function. We'll discuss permutation importance and partial dependence plots.\n",
    "\n",
    "### Approach 1: Mean decrease impurity (and coeficients in a linear model).\n",
    "\n",
    "- See https://papers.nips.cc/paper/4928-understanding-variable-importances-in-forests-of-randomized-trees.pdf equation 2\n",
    "- Ask *what prediction functions would support this definition of feature importance?* (A: tree based)\n",
    "- Work through the sklearn iris example below.\n",
    "- Then ask *when might this fail* as a turn and talk.\n",
    "- Finally show the correlated features example below for both elastic net (circle back to our theorem) and GBM, and hit (i.e. screenshot) the Eli5 warning above.\n",
    "\n",
    "\n",
    "### Approach 2: Permutation Importance\n",
    "\n",
    "- Can use permutations to get a handle on the importance of a feature:\n",
    "    - From original Breiman paper:\n",
    "    Suppose there are $M$ input variables. After each tree is constructed, the values of the $m^{th}$ variable in the out-of-bag examples are randomly permuted and the out-of-bag data is run down the corresponding tree. The classification given for each $x_n$ that is out of bag is saved. This is repeated for $m = 1, 2,\\cdots, M$. At the end of the run, the plurality of out-of-bag class votes for $x_n$ with the $m^{th}$ variable noised up is compared with the true class label of $x_n$ to give a misclassification rate.\n",
    "    - Show application to a single tree using http://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html\n",
    "    - Ask *based on the tree, why would permuting petal length lead to a larger decrease in performance?* -- answer:  See first split\n",
    "\n",
    "##### Extensions\n",
    "- Note we can extend this idea from permutations over features to permutations over the target to get a handle on the significance of a test score -- see http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.permutation_test_score.html\n",
    "\n",
    "\n",
    "##### Pros:\n",
    "- Fully general (not we're not making any assumptions on the algorithm except for OOB sample (in Breiman paper) -- which is just to exploit OOB structure of RF -- could just use a validation set).\n",
    "- Computationally expensive relative to model-specific (extract betas).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Plants Database\n",
      "====================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML iris datasets.\n",
      "http://archive.ics.uci.edu/ml/datasets/Iris\n",
      "\n",
      "The famous Iris database, first used by Sir R.A Fisher\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "References\n",
      "----------\n",
      "   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "iris = load_iris()\n",
    "clf = tree.DecisionTreeClassifier(max_depth=3)\n",
    "clf = clf.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Some notes for speaker view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"da407ed0a374aaea\"></div>\n",
       "    <script type=\"text/javascript\">\n",
       "        $(function(){\n",
       "            var p = $(\"#da407ed0a374aaea\");\n",
       "            if (p.length==0) return;\n",
       "\n",
       "            while (!p.hasClass(\"cell\")) {\n",
       "                p=p.parent();\n",
       "\n",
       "                if (p.prop(\"tagName\") ==\"body\") return;\n",
       "            }\n",
       "            var cell = p;\n",
       "            cell.find(\".input\").addClass(\"hide-in-slideshow\")\n",
       "        });\n",
       "    </script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"560pt\" height=\"414pt\"\n",
       " viewBox=\"0.00 0.00 559.83 414.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 410)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-410 555.831,-410 555.831,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M260.609,-406C260.609,-406 135.146,-406 135.146,-406 129.146,-406 123.146,-400 123.146,-394 123.146,-394 123.146,-340 123.146,-340 123.146,-334 129.146,-328 135.146,-328 135.146,-328 260.609,-328 260.609,-328 266.609,-328 272.609,-334 272.609,-340 272.609,-340 272.609,-394 272.609,-394 272.609,-400 266.609,-406 260.609,-406\"/>\n",
       "<text text-anchor=\"start\" x=\"131.012\" y=\"-390.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 0.8</text>\n",
       "<text text-anchor=\"start\" x=\"161.486\" y=\"-376.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.667</text>\n",
       "<text text-anchor=\"start\" x=\"152.155\" y=\"-362.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 150</text>\n",
       "<text text-anchor=\"start\" x=\"138.138\" y=\"-348.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 50, 50]</text>\n",
       "<text text-anchor=\"start\" x=\"153.328\" y=\"-334.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M167.784,-285C167.784,-285 71.9712,-285 71.9712,-285 65.9712,-285 59.9712,-279 59.9712,-273 59.9712,-273 59.9712,-233 59.9712,-233 59.9712,-227 65.9712,-221 71.9712,-221 71.9712,-221 167.784,-221 167.784,-221 173.784,-221 179.784,-227 179.784,-233 179.784,-233 179.784,-273 179.784,-273 179.784,-279 173.784,-285 167.784,-285\"/>\n",
       "<text text-anchor=\"start\" x=\"91.2725\" y=\"-269.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"78.0483\" y=\"-255.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n",
       "<text text-anchor=\"start\" x=\"67.9243\" y=\"-241.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 0, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"75.3276\" y=\"-227.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M171.252,-327.769C163.515,-316.66 155.053,-304.509 147.27,-293.333\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"150.061,-291.216 141.474,-285.01 144.317,-295.216 150.061,-291.216\"/>\n",
       "<text text-anchor=\"middle\" x=\"137.07\" y=\"-305.419\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M343.396,-292C343.396,-292 210.359,-292 210.359,-292 204.359,-292 198.359,-286 198.359,-280 198.359,-280 198.359,-226 198.359,-226 198.359,-220 204.359,-214 210.359,-214 210.359,-214 343.396,-214 343.396,-214 349.396,-214 355.396,-220 355.396,-226 355.396,-226 355.396,-280 355.396,-280 355.396,-286 349.396,-292 343.396,-292\"/>\n",
       "<text text-anchor=\"start\" x=\"206.119\" y=\"-276.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.75</text>\n",
       "<text text-anchor=\"start\" x=\"248.272\" y=\"-262.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"231.155\" y=\"-248.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 100</text>\n",
       "<text text-anchor=\"start\" x=\"221.031\" y=\"-234.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 50, 50]</text>\n",
       "<text text-anchor=\"start\" x=\"223\" y=\"-220.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M224.844,-327.769C231.072,-318.939 237.765,-309.451 244.207,-300.318\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"247.129,-302.248 250.033,-292.058 241.409,-298.213 247.129,-302.248\"/>\n",
       "<text text-anchor=\"middle\" x=\"254.289\" y=\"-312.494\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\n",
       "<path fill=\"#39e581\" fill-opacity=\"0.898039\" stroke=\"black\" d=\"M256.357,-178C256.357,-178 117.398,-178 117.398,-178 111.398,-178 105.398,-172 105.398,-166 105.398,-166 105.398,-112 105.398,-112 105.398,-106 111.398,-100 117.398,-100 117.398,-100 256.357,-100 256.357,-100 262.357,-100 268.357,-106 268.357,-112 268.357,-112 268.357,-166 268.357,-166 268.357,-172 262.357,-178 256.357,-178\"/>\n",
       "<text text-anchor=\"start\" x=\"113.388\" y=\"-162.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.95</text>\n",
       "<text text-anchor=\"start\" x=\"150.486\" y=\"-148.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.168</text>\n",
       "<text text-anchor=\"start\" x=\"145.048\" y=\"-134.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 54</text>\n",
       "<text text-anchor=\"start\" x=\"134.924\" y=\"-120.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 49, 5]</text>\n",
       "<text text-anchor=\"start\" x=\"133\" y=\"-106.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\"><title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M246.156,-213.769C238.988,-204.849 231.281,-195.257 223.872,-186.038\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"226.452,-183.661 217.46,-178.058 220.996,-188.046 226.452,-183.661\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\"><title>6</title>\n",
       "<path fill=\"#8139e5\" fill-opacity=\"0.976471\" stroke=\"black\" d=\"M437.357,-178C437.357,-178 298.398,-178 298.398,-178 292.398,-178 286.398,-172 286.398,-166 286.398,-166 286.398,-112 286.398,-112 286.398,-106 292.398,-100 298.398,-100 298.398,-100 437.357,-100 437.357,-100 443.357,-100 449.357,-106 449.357,-112 449.357,-112 449.357,-166 449.357,-166 449.357,-172 443.357,-178 437.357,-178\"/>\n",
       "<text text-anchor=\"start\" x=\"294.388\" y=\"-162.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.85</text>\n",
       "<text text-anchor=\"start\" x=\"331.486\" y=\"-148.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.043</text>\n",
       "<text text-anchor=\"start\" x=\"326.048\" y=\"-134.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 46</text>\n",
       "<text text-anchor=\"start\" x=\"315.924\" y=\"-120.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 45]</text>\n",
       "<text text-anchor=\"start\" x=\"318.276\" y=\"-106.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\"><title>2&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M307.94,-213.769C315.188,-204.849 322.981,-195.257 330.471,-186.038\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"333.365,-188.027 336.955,-178.058 327.933,-183.613 333.365,-188.027\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\n",
       "<path fill=\"#39e581\" fill-opacity=\"0.980392\" stroke=\"black\" d=\"M111.633,-64C111.633,-64 12.1223,-64 12.1223,-64 6.12232,-64 0.122316,-58 0.122316,-52 0.122316,-52 0.122316,-12 0.122316,-12 0.122316,-6 6.12232,-0 12.1223,-0 12.1223,-0 111.633,-0 111.633,-0 117.633,-0 123.633,-6 123.633,-12 123.633,-12 123.633,-52 123.633,-52 123.633,-58 117.633,-64 111.633,-64\"/>\n",
       "<text text-anchor=\"start\" x=\"25.4863\" y=\"-48.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.041</text>\n",
       "<text text-anchor=\"start\" x=\"20.0483\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 48</text>\n",
       "<text text-anchor=\"start\" x=\"9.92432\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 47, 1]</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\"><title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M141.396,-99.7956C130.053,-90.2671 117.899,-80.0585 106.593,-70.5614\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"108.83,-67.8692 98.9217,-64.1172 104.328,-73.2291 108.83,-67.8692\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\"><title>5</title>\n",
       "<path fill=\"#8139e5\" fill-opacity=\"0.498039\" stroke=\"black\" d=\"M244.581,-64C244.581,-64 153.174,-64 153.174,-64 147.174,-64 141.174,-58 141.174,-52 141.174,-52 141.174,-12 141.174,-12 141.174,-6 147.174,-0 153.174,-0 153.174,-0 244.581,-0 244.581,-0 250.581,-0 256.581,-6 256.581,-12 256.581,-12 256.581,-52 256.581,-52 256.581,-58 250.581,-64 244.581,-64\"/>\n",
       "<text text-anchor=\"start\" x=\"162.486\" y=\"-48.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n",
       "<text text-anchor=\"start\" x=\"160.941\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n",
       "<text text-anchor=\"start\" x=\"150.817\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 4]</text>\n",
       "<text text-anchor=\"start\" x=\"149.276\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\"><title>3&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M191.244,-99.7956C192.197,-91.4581 193.209,-82.6 194.174,-74.1534\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"197.663,-74.4499 195.321,-64.1172 190.708,-73.6551 197.663,-74.4499\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\"><title>7</title>\n",
       "<path fill=\"#8139e5\" fill-opacity=\"0.498039\" stroke=\"black\" d=\"M401.581,-64C401.581,-64 310.174,-64 310.174,-64 304.174,-64 298.174,-58 298.174,-52 298.174,-52 298.174,-12 298.174,-12 298.174,-6 304.174,-0 310.174,-0 310.174,-0 401.581,-0 401.581,-0 407.581,-0 413.581,-6 413.581,-12 413.581,-12 413.581,-52 413.581,-52 413.581,-58 407.581,-64 401.581,-64\"/>\n",
       "<text text-anchor=\"start\" x=\"319.486\" y=\"-48.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n",
       "<text text-anchor=\"start\" x=\"317.941\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n",
       "<text text-anchor=\"start\" x=\"307.817\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 2]</text>\n",
       "<text text-anchor=\"start\" x=\"306.276\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\"><title>6&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M363.511,-99.7956C362.558,-91.4581 361.546,-82.6 360.581,-74.1534\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"364.047,-73.6551 359.434,-64.1172 357.092,-74.4499 364.047,-73.6551\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\"><title>8</title>\n",
       "<path fill=\"#8139e5\" stroke=\"black\" d=\"M539.784,-64C539.784,-64 443.971,-64 443.971,-64 437.971,-64 431.971,-58 431.971,-52 431.971,-52 431.971,-12 431.971,-12 431.971,-6 437.971,-0 443.971,-0 443.971,-0 539.784,-0 539.784,-0 545.784,-0 551.784,-6 551.784,-12 551.784,-12 551.784,-52 551.784,-52 551.784,-58 545.784,-64 539.784,-64\"/>\n",
       "<text text-anchor=\"start\" x=\"463.272\" y=\"-48.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"450.048\" y=\"-34.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 43</text>\n",
       "<text text-anchor=\"start\" x=\"439.924\" y=\"-20.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 43]</text>\n",
       "<text text-anchor=\"start\" x=\"442.276\" y=\"-6.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\"><title>6&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M412.995,-99.7956C424.14,-90.3587 436.072,-80.2547 447.196,-70.8355\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"449.76,-73.2504 455.13,-64.1172 445.236,-67.9083 449.76,-73.2504\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x10f5f18d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hide_code_in_slideshow()\n",
    "\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                                feature_names=iris.feature_names,  \n",
    "                                class_names=iris.target_names,  \n",
    "                                filled=True, rounded=True,  \n",
    "                                special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sepal width (cm)</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>petal length (cm)</td>\n",
       "      <td>0.053936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>petal width (cm)</td>\n",
       "      <td>0.946064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Feature  Importance\n",
       "0  sepal length (cm)    0.000000\n",
       "1   sepal width (cm)    0.000000\n",
       "2  petal length (cm)    0.053936\n",
       "3   petal width (cm)    0.946064"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame({'Feature': iris.feature_names,\n",
    "              'Importance':clf.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recall a tree is built from a learning sample of size $N$ drawn from\n",
    "$P(X_1,\\cdots, X_p, Y)$ using a recursive procedure which identifies at each node $t$ the split $s_t = s^∗$ for which the partition of the $N_t$ node samples into $t_L$ and $t_R$ maximizes the decrease\n",
    "$$\\Delta i(s, t) = i(t) − p_Li(t_L) − p_Ri(t_R)$$\n",
    "of some impurity measure $i(t)$ (e.g., the Gini index, the Shannon entropy, or the variance of $Y$).\n",
    "\n",
    "This suggests a \"Mean Decrease Impurity\" importance measure, here shown for all trees in forest with $T$ trees:\n",
    "$$Imp(X_m) = \\frac{1}{N_T}\\sum_T\\sum_{t \\in T:v(s_t)=X_m}p(t)\\Delta i(s_t,t)$$\n",
    "Note $p(t) = N_t/N$ is the proportion of samples reaching node $t$ and $v(s_t)$ is the variable used in split $s_t$.\n",
    "\n",
    "Let's calculate this explicitly for sepal length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_single_tree(tree, feature_names=iris.feature_names):\n",
    "    total_samples = np.sum(tree.weighted_n_node_samples)\n",
    "    feature_importance = defaultdict(float)\n",
    "    # See sklearn example plot_unveil_tree_structure.html\n",
    "    is_leaf = (tree.children_right == tree.children_left)\n",
    "    for ix in range(len(is_leaf)):\n",
    "        if not is_leaf[ix]:\n",
    "            impurity = tree.impurity[ix]\n",
    "            split_feature = tree.feature[ix]\n",
    "            num_at_node = tree.weighted_n_node_samples[ix]\n",
    "\n",
    "            # Get left child contribution\n",
    "            left_child = tree.children_left[ix]\n",
    "            left_decrease = tree.weighted_n_node_samples[left_child]/num_at_node * \\\n",
    "                                tree.impurity[left_child]\n",
    "\n",
    "            # Get right child contribution\n",
    "            right_child = tree.children_right[ix]\n",
    "            right_decrease = tree.weighted_n_node_samples[right_child]/num_at_node * \\\n",
    "                                tree.impurity[right_child]\n",
    "\n",
    "            delta = impurity - left_decrease - right_decrease\n",
    "            \n",
    "            feature_importance[feature_names[split_feature]] \\\n",
    "                += num_at_node / total_samples * delta\n",
    "    norm = np.sum(feature_importance.values())\n",
    "    feature_importance = {key: val/norm for key, val in feature_importance.items()}\n",
    "    return feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'petal length (cm)': 0.053936331199339008,\n",
       " 'petal width (cm)': 0.94606366880066095}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_single_tree(clf.tree_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we can easily extend this to ensemble of trees :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>0.096164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sepal width (cm)</td>\n",
       "      <td>0.025964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>petal length (cm)</td>\n",
       "      <td>0.449715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>petal width (cm)</td>\n",
       "      <td>0.428157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Feature  Importance\n",
       "0  sepal length (cm)    0.096164\n",
       "1   sepal width (cm)    0.025964\n",
       "2  petal length (cm)    0.449715\n",
       "3   petal width (cm)    0.428157"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'Feature': iris.feature_names,\n",
    "              'Importance':forest.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_importance = defaultdict(float)\n",
    "for tree in forest.estimators_:\n",
    "    tree_importance = feature_importance_single_tree(tree.tree_)\n",
    "    \n",
    "    for key, val in tree_importance.items():\n",
    "        forest_importance[key] += val\n",
    "forest_importance = {key:val/len(forest.estimators_)\\\n",
    "                     for key, val in forest_importance.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'petal length (cm)': 0.44971549580809772,\n",
       " 'petal width (cm)': 0.42815674228676259,\n",
       " 'sepal length (cm)': 0.096163772856607432,\n",
       " 'sepal width (cm)': 0.025963989048532574}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is permutation importance, per the Breiman paper -- here we just permute each feature in turn and see how the accuracy degrades. In the original paper, this was done over the OOB sample, but we can also do it for a test set (here we just use the training data as a toy examle). Note we don't have to use accuracy -- any score is fine -- but our sklearn decision tree score method reurns accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "perm = PermutationImportance(clf)\n",
    "perm.fit(iris.data, iris.target)\n",
    "output = eli5.show_weights(perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sepal width (cm)</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>petal length (cm)</td>\n",
       "      <td>0.105333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>petal width (cm)</td>\n",
       "      <td>0.525333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Feature  Importance\n",
       "0  sepal length (cm)    0.000000\n",
       "1   sepal width (cm)    0.000000\n",
       "2  petal length (cm)    0.105333\n",
       "3   petal width (cm)    0.525333"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame({'Feature': iris.feature_names, 'Importance':perm.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(forest)\n",
    "perm.fit(iris.data, iris.target)\n",
    "output = eli5.show_weights(perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sepal length (cm)</td>\n",
       "      <td>0.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sepal width (cm)</td>\n",
       "      <td>0.013333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>petal length (cm)</td>\n",
       "      <td>0.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>petal width (cm)</td>\n",
       "      <td>0.185333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Feature  Importance\n",
       "0  sepal length (cm)    0.012000\n",
       "1   sepal width (cm)    0.013333\n",
       "2  petal length (cm)    0.216000\n",
       "3   petal width (cm)    0.185333"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'Feature': iris.feature_names, 'Importance':perm.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When this might fail\n",
    "\n",
    "Let's look at an example with correlated features using both elastic net and feature importance (from a GBM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1000\n",
    "X1 = np.random.uniform(-10,10,size)\n",
    "Z1 = np.random.uniform(-10,10,size)\n",
    "X2 = Z1 + np.random.normal(0,2,size)\n",
    "X3 = Z1 + np.random.normal(0,2,size)\n",
    "X4 = Z1 + np.random.normal(0,2,size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_function(X1,Z1):\n",
    "    return -1./2.*X1 + Z1 + np.random.normal(0,1,len(X1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here ask *which feature is more important?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = actual_function(X1,Z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = np.stack([X1,X2,X3,X4]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031228</td>\n",
       "      <td>0.039624</td>\n",
       "      <td>0.051494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.031228</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.882811</td>\n",
       "      <td>0.880844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.039624</td>\n",
       "      <td>0.882811</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.887544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.051494</td>\n",
       "      <td>0.880844</td>\n",
       "      <td>0.887544</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3\n",
       "0  1.000000  0.031228  0.039624  0.051494\n",
       "1  0.031228  1.000000  0.882811  0.880844\n",
       "2  0.039624  0.882811  1.000000  0.887544\n",
       "3  0.051494  0.880844  0.887544  1.000000"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(feats).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X, test_X, train_y, test_y = train_test_split(feats,Y,test_size=0.2,\n",
    "                                                   random_state=1345134)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = GradientBoostingRegressor(n_estimators=200,subsample=0.5)\n",
    "grid = GridSearchCV(reg,\n",
    "                    param_grid={\n",
    "                        'max_leaf_nodes':[10,25,50],\n",
    "                        'min_samples_leaf':[10,25,50]\n",
    "                    }, n_jobs=5,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=200, presort='auto', random_state=None,\n",
       "             subsample=0.5, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=5,\n",
       "       param_grid={'max_leaf_nodes': [10, 25, 50], 'min_samples_leaf': [10, 25, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.28739322,  0.22013966,  0.23425943,  0.25820768])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92819576926216174"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.score(test_X,test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(alphas=None, copy_X=True, cv=None, eps=0.001, fit_intercept=True,\n",
       "       l1_ratio=0.5, max_iter=1000, n_alphas=100, n_jobs=1,\n",
       "       normalize=False, positive=False, precompute='auto',\n",
       "       random_state=None, selection='cyclic', tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enet = ElasticNetCV()\n",
    "enet.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94159265017916305"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enet.score(test_X,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.49495418,  0.29880776,  0.35362496,  0.305685  ])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enet.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95811772091903613"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enet.coef_[1:].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask the questions:\n",
    "- How are these similar/different?\n",
    "    - Coefficients give us a sign -- importance does not.\n",
    "    - Both show X1 as more important than the highly correlated X2..X4\n",
    "    - This (could be) a problem if you were to:\n",
    "        - Try to use these feature importance measures to try to explain the prediction function (i.e. saying the most important feature is X1 is misleading -- the partial of the true conditional expectation function is 2x greater for the latent variable Z1 than for the observed variable X1).\n",
    "        - Using `SelectFromModel` for feature selection -- imagine extending this example to construct a more pathalogical case where some `SelectFromModel` threshold excludes a set of highly correlated but informative variables (note you would want to avoid this by doing some search/optimization over the `SelectFromModel` threshold hyperparameter).\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now with identical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = np.stack([X1,Z1,Z1,Z1]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.001165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001165</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001165</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001165</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3\n",
       "0  1.000000  0.001165  0.001165  0.001165\n",
       "1  0.001165  1.000000  1.000000  1.000000\n",
       "2  0.001165  1.000000  1.000000  1.000000\n",
       "3  0.001165  1.000000  1.000000  1.000000"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(feats).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=200, presort='auto', random_state=None,\n",
       "             subsample=0.5, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=5,\n",
       "       param_grid={'max_leaf_nodes': [10, 25, 50], 'min_samples_leaf': [10, 25, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(feats, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.50083372,  0.16263472,  0.17598738,  0.16054417])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_.feature_importances_"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "livereveal": {
   "scroll": true,
   "theme": "moon",
   "transition": "slide"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
